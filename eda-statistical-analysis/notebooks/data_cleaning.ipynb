{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08a4af0a-c74a-4e74-93ba-cda193dfb4c3",
   "metadata": {},
   "source": [
    "# Heart disease data cleaning\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "- the ability to extract dataset from a CSV\n",
    "- the ability to inspect the data\n",
    "- the ability to clean the data. Since this data was large, we simply dropped rows with incomplete data as the focus is on data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7083265-d02e-4f61-9765-96289d509476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas and define path for heart failure data\n",
    "import pandas as pd\n",
    "\n",
    "heart_failure_csv_path = '../data/heart_disease_uci.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4f357dd-e162-497f-b571-426b4fdc15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target column\n",
    "\n",
    "target_column = 'num'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f3ddffe-5e54-4566-b79b-bef0fc88c648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded CSV file: ../data/heart_disease_uci.csv\n",
      "Loaded CSV shape: (920, 16)\n",
      "CSV columns found: ['id', 'age', 'sex', 'dataset', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalch', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num']\n"
     ]
    }
   ],
   "source": [
    "# load csv\n",
    "\n",
    "try:\n",
    "    #read the CSV file\n",
    "    heart_failure_df = pd.read_csv(heart_failure_csv_path, sep=',')\n",
    "\n",
    "    print(f\"Successfully loaded CSV file: {heart_failure_csv_path}\")\n",
    "    print(f\"Loaded CSV shape: {heart_failure_df.shape}\")\n",
    "    print(f\"CSV columns found: {heart_failure_df.columns.tolist()}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: CSV file not found at path: {heart_failure_csv_path}\")\n",
    "    print(\"Please double check the file path\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occured loading or processing the CSV: {e}\")\n",
    "    print(\"Please check file path, format (is it truly CSV?), separator, header row, and column name.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be3930f7-371f-485d-8527-f93c0c47258c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 920 entries, 0 to 919\n",
      "Data columns (total 16 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   id        920 non-null    int64  \n",
      " 1   age       920 non-null    int64  \n",
      " 2   sex       920 non-null    object \n",
      " 3   dataset   920 non-null    object \n",
      " 4   cp        920 non-null    object \n",
      " 5   trestbps  861 non-null    float64\n",
      " 6   chol      890 non-null    float64\n",
      " 7   fbs       830 non-null    object \n",
      " 8   restecg   918 non-null    object \n",
      " 9   thalch    865 non-null    float64\n",
      " 10  exang     865 non-null    object \n",
      " 11  oldpeak   858 non-null    float64\n",
      " 12  slope     611 non-null    object \n",
      " 13  ca        309 non-null    float64\n",
      " 14  thal      434 non-null    object \n",
      " 15  num       920 non-null    int64  \n",
      "dtypes: float64(5), int64(3), object(8)\n",
      "memory usage: 115.1+ KB\n",
      "None\n",
      "               id         age    trestbps        chol      thalch     oldpeak  \\\n",
      "count  920.000000  920.000000  861.000000  890.000000  865.000000  858.000000   \n",
      "mean   460.500000   53.510870  132.132404  199.130337  137.545665    0.878788   \n",
      "std    265.725422    9.424685   19.066070  110.780810   25.926276    1.091226   \n",
      "min      1.000000   28.000000    0.000000    0.000000   60.000000   -2.600000   \n",
      "25%    230.750000   47.000000  120.000000  175.000000  120.000000    0.000000   \n",
      "50%    460.500000   54.000000  130.000000  223.000000  140.000000    0.500000   \n",
      "75%    690.250000   60.000000  140.000000  268.000000  157.000000    1.500000   \n",
      "max    920.000000   77.000000  200.000000  603.000000  202.000000    6.200000   \n",
      "\n",
      "               ca         num  \n",
      "count  309.000000  920.000000  \n",
      "mean     0.676375    0.995652  \n",
      "std      0.935653    1.142693  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    0.000000  \n",
      "50%      0.000000    1.000000  \n",
      "75%      1.000000    2.000000  \n",
      "max      3.000000    4.000000  \n"
     ]
    }
   ],
   "source": [
    "# initial data analysis using descriptive statistics\n",
    "\n",
    "print(heart_failure_df.info())\n",
    "print(heart_failure_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a869564-2e02-4d79-9dce-6861c679a7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Columns\n",
      "\n",
      "id (920 unique values)\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920]\n",
      "\n",
      "age (50 unique values)\n",
      "[28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77]\n",
      "\n",
      "trestbps (62 unique values)\n",
      "[0.0, 80.0, 92.0, 94.0, 95.0, 96.0, 98.0, 100.0, 101.0, 102.0, 104.0, 105.0, 106.0, 108.0, 110.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 120.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 132.0, 134.0, 135.0, 136.0, 138.0, 140.0, 142.0, 144.0, 145.0, 146.0, 148.0, 150.0, 152.0, 154.0, 155.0, 156.0, 158.0, 160.0, 164.0, 165.0, 170.0, 172.0, 174.0, 178.0, 180.0, 185.0, 190.0, 192.0, 200.0, nan]\n",
      "\n",
      "chol (218 unique values)\n",
      "[126.0, 131.0, 132.0, 147.0, 149.0, 157.0, 160.0, 161.0, 166.0, 169.0, 173.0, 176.0, 178.0, 180.0, 184.0, 187.0, 193.0, 194.0, 195.0, 196.0, 200.0, 207.0, 210.0, 214.0, 215.0, 218.0, 223.0, 228.0, 237.0, 238.0, 241.0, 242.0, 244.0, 259.0, 262.0, 271.0, 277.0, 278.0, 291.0, 292.0, 295.0, 297.0, 306.0, 311.0, 313.0, 315.0, 319.0, 327.0, 339.0, 342.0, 358.0, 394.0, 409.0, nan, 0.0, 85.0, 100.0, 117.0, 129.0, 139.0, 141.0, 142.0, 153.0, 156.0, 163.0, 164.0, 165.0, 167.0, 168.0, 170.0, 171.0, 172.0, 174.0, 175.0, 177.0, 179.0, 181.0, 182.0, 183.0, 185.0, 186.0, 188.0, 190.0, 192.0, 197.0, 198.0, 199.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 208.0, 209.0, 211.0, 212.0, 213.0, 216.0, 217.0, 219.0, 220.0, 221.0, 222.0, 224.0, 225.0, 226.0, 227.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 239.0, 240.0, 243.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 260.0, 261.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 272.0, 273.0, 274.0, 275.0, 276.0, 279.0, 280.0, 281.0, 282.0, 283.0, 284.0, 285.0, 286.0, 287.0, 288.0, 289.0, 290.0, 293.0, 294.0, 298.0, 299.0, 300.0, 302.0, 303.0, 304.0, 305.0, 307.0, 308.0, 309.0, 310.0, 312.0, 316.0, 318.0, 320.0, 321.0, 322.0, 325.0, 326.0, 328.0, 329.0, 330.0, 331.0, 333.0, 335.0, 336.0, 337.0, 338.0, 340.0, 341.0, 344.0, 347.0, 349.0, 353.0, 354.0, 355.0, 360.0, 365.0, 369.0, 384.0, 385.0, 388.0, 392.0, 393.0, 404.0, 407.0, 412.0, 417.0, 458.0, 466.0, 468.0, 491.0, 518.0, 529.0, 564.0, 603.0]\n",
      "\n",
      "thalch (120 unique values)\n",
      "[60.0, 63.0, 67.0, 69.0, 70.0, 71.0, 72.0, 73.0, 77.0, 78.0, 80.0, 82.0, 83.0, 84.0, 86.0, 87.0, 88.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 184.0, 185.0, 186.0, 187.0, 188.0, 190.0, 192.0, 194.0, 195.0, 202.0, nan]\n",
      "\n",
      "oldpeak (54 unique values)\n",
      "[-2.6, -2.0, -1.5, -1.1, -1.0, -0.9, -0.8, -0.7, -0.5, -0.1, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.8, 2.9, 3.0, 3.1, 3.2, 3.4, 3.5, 3.6, 3.7, 3.8, 4.0, 4.2, 4.4, 5.0, 5.6, 6.2, nan]\n",
      "\n",
      "ca (5 unique values)\n",
      "[0.0, 1.0, 2.0, 3.0, nan]\n",
      "\n",
      "num (5 unique values)\n",
      "[0, 1, 2, 3, 4]\n",
      "Categorical Columns\n",
      "\n",
      "sex - Value Counts\n",
      "sex\n",
      "Male      726\n",
      "Female    194\n",
      "Name: count, dtype: int64\n",
      "\n",
      "dataset - Value Counts\n",
      "dataset\n",
      "Cleveland        304\n",
      "Hungary          293\n",
      "VA Long Beach    200\n",
      "Switzerland      123\n",
      "Name: count, dtype: int64\n",
      "\n",
      "cp - Value Counts\n",
      "cp\n",
      "asymptomatic       496\n",
      "non-anginal        204\n",
      "atypical angina    174\n",
      "typical angina      46\n",
      "Name: count, dtype: int64\n",
      "\n",
      "fbs - Value Counts\n",
      "fbs\n",
      "False    692\n",
      "True     138\n",
      "NaN       90\n",
      "Name: count, dtype: int64\n",
      "\n",
      "restecg - Value Counts\n",
      "restecg\n",
      "normal              551\n",
      "lv hypertrophy      188\n",
      "st-t abnormality    179\n",
      "NaN                   2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "exang - Value Counts\n",
      "exang\n",
      "False    528\n",
      "True     337\n",
      "NaN       55\n",
      "Name: count, dtype: int64\n",
      "\n",
      "slope - Value Counts\n",
      "slope\n",
      "flat           345\n",
      "NaN            309\n",
      "upsloping      203\n",
      "downsloping     63\n",
      "Name: count, dtype: int64\n",
      "\n",
      "thal - Value Counts\n",
      "thal\n",
      "NaN                   486\n",
      "normal                196\n",
      "reversable defect     191\n",
      "fixed defect           46\n",
      "reversable defEect      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# explore missing values for cleaning\n",
    "\n",
    "def print_unique_values(df):\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "    print(\"Numerical Columns\")\n",
    "    for col in numerical_cols:\n",
    "        unique_vals = sorted(df[col].unique())\n",
    "        print(f\"\\n{col} ({len(unique_vals)} unique values)\")\n",
    "        print(unique_vals)\n",
    "\n",
    "    print(\"Categorical Columns\")\n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\n{col} - Value Counts\")\n",
    "        print(df[col].value_counts(dropna=False))\n",
    "\n",
    "print_unique_values(heart_failure_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e1bffa-892d-4f67-a701-1cc25cf27764",
   "metadata": {},
   "source": [
    "### Notes from analysis:\n",
    "\n",
    "##### 1) trestbps and chol have 2 forms of missing values: nan and 0.0. It is physiologically impossible for a living patient to have a resting blood pressure or cholesterol level of 0.\n",
    "##### 2) all other missing value types are nan\n",
    "##### 3) presence of heart disease is labeled 0 (no presence) or 1-4 (presence), need to create simple binary output, 0 for no presence and 1 for presence\n",
    "\n",
    "#### As the focus of this is visualization of data and statistical analysis, rows with data missing will just be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a2ee9aa-f0b4-46c6-ab88-1ee9d7bf4718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 621 rows with missing values.\n",
      "Dataframe shape is: (299, 16)\n"
     ]
    }
   ],
   "source": [
    "# drop rows with any missing values\n",
    "rows_before = heart_failure_df.shape[0]\n",
    "heart_failure_df.dropna(inplace=True)\n",
    "heart_failure_df = heart_failure_df[(heart_failure_df['trestbps'] != 0.0) & (heart_failure_df['chol'] != 0.0)]\n",
    "rows_after = heart_failure_df.shape[0]\n",
    "print(f\"Removed {rows_before - rows_after} rows with missing values.\")\n",
    "print(f\"Dataframe shape is: {heart_failure_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "744beba8-11ec-45cd-8d41-ca32f8199799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after cleaning:\n",
      "id          0\n",
      "age         0\n",
      "sex         0\n",
      "dataset     0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalch      0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          0\n",
      "thal        0\n",
      "num         0\n",
      "dtype: int64\n",
      "No duplicate rows found.\n"
     ]
    }
   ],
   "source": [
    "# check that there are no missing values\n",
    "print(\"Missing values after cleaning:\")\n",
    "print(heart_failure_df.isnull().sum())\n",
    "# check for duplicates\n",
    "duplicates = heart_failure_df.duplicated().sum()\n",
    "if duplicates > 0:\n",
    "    print(f\"Found {duplicates} duplicate rows.\")\n",
    "    heart_failure_df.drop_duplicates(inplace=True)\n",
    "    print(f\"Removed {duplicates} duplicate rows.\")\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf0027b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target column 'num' is not binary. It has 5 unique values.\n"
     ]
    }
   ],
   "source": [
    "# check to see if the target column is binary\n",
    "if heart_failure_df[target_column].nunique() > 2:\n",
    "    print(f\"Target column '{target_column}' is not binary. It has {heart_failure_df[target_column].nunique()} unique values.\")\n",
    "else:\n",
    "    print(f\"Target column '{target_column}' is binary. It has {heart_failure_df[target_column].nunique()} unique values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cf88729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 'num' column to binary values.\n",
      "num\n",
      "0    160\n",
      "1    139\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert \"num\" column values to binary (0 or 1)\n",
    "heart_failure_df['num'] = heart_failure_df['num'].apply(lambda x: 0 if x == 0 else 1)\n",
    "print(\"Converted 'num' column to binary values.\")\n",
    "print(heart_failure_df['num'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f761d612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleaned dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 299 entries, 0 to 748\n",
      "Data columns (total 16 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   id        299 non-null    int64  \n",
      " 1   age       299 non-null    int64  \n",
      " 2   sex       299 non-null    object \n",
      " 3   dataset   299 non-null    object \n",
      " 4   cp        299 non-null    object \n",
      " 5   trestbps  299 non-null    float64\n",
      " 6   chol      299 non-null    float64\n",
      " 7   fbs       299 non-null    object \n",
      " 8   restecg   299 non-null    object \n",
      " 9   thalch    299 non-null    float64\n",
      " 10  exang     299 non-null    object \n",
      " 11  oldpeak   299 non-null    float64\n",
      " 12  slope     299 non-null    object \n",
      " 13  ca        299 non-null    float64\n",
      " 14  thal      299 non-null    object \n",
      " 15  num       299 non-null    int64  \n",
      "dtypes: float64(5), int64(3), object(8)\n",
      "memory usage: 39.7+ KB\n",
      "None\n",
      "Final cleaned dataset shape:\n",
      "(299, 16)\n",
      "heart_failure_df.describe():\n",
      "Cleaned dataset saved to: ../data/heart_disease_uci_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# final check for cleaned dataset\n",
    "print(\"Final cleaned dataset info:\")\n",
    "print(heart_failure_df.info())\n",
    "print(\"Final cleaned dataset shape:\")\n",
    "print(heart_failure_df.shape)\n",
    "print(\"heart_failure_df.describe():\")\n",
    "# save cleaned dataset to CSV\n",
    "cleaned_heart_failure_csv_path = '../data/heart_disease_uci_cleaned.csv'\n",
    "try:\n",
    "    heart_failure_df.to_csv(cleaned_heart_failure_csv_path, index=False)\n",
    "    print(f\"Cleaned dataset saved to: {cleaned_heart_failure_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the cleaned dataset: {e}\")\n",
    "    print(\"Please check file path and permissions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f9eded",
   "metadata": {},
   "source": [
    "### Final notes\n",
    "\n",
    "In this notebook, we performed basic data exploration and cleaning. We identified that nan were not the only missing values for a couple continuous variables (blood pressure and cholesterol), and that the presence of heart disease was not binary. The steps we took to clean the data were:\n",
    "\n",
    "1) drop rows where data was nan\n",
    "2) ensure our updated dataframe did not have any values of 0.0 for blood pressure or cholesterol\n",
    "3) converted our target variable to binary values, with 0 representing no presence of heart disease and 1 to represent heart disease\n",
    "\n",
    "Next steps will be visualization of the data and statistical analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32724f0-94d9-4ebe-82f7-564348bb216f",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
